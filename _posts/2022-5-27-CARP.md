---
layout: post
title: Overview of Cache Array Routing Protocol (CARP)
---


### Background
[Sharding](https://en.wikipedia.org/wiki/Shard_(database_architecture)) is a mechanism for distributing data or load in distributed systems design.  One described method for sharding or load balancing HTTP requests across proxy servers is ["Cache Array Routing Protocol"](https://en.wikipedia.org/wiki/Cache_Array_Routing_Protocol) (CARP).

CARP itself is form of [weighted](https://en.wikipedia.org/wiki/Rendezvous_hashing#Weighted_variations) [Rendezvous Hashing](https://en.wikipedia.org/wiki/Rendezvous_hashing), (also referred to as highest random weight (HRW)) which (from wiki):

- ```allows clients to achieve distributed agreement on a set of k options out of a possible set of n options.```

The general algorithm for:

1. calculating load factors (per server) using relative weights
2. selection based on uri hash + server hash + load factor

is described in the IETF draft: [https://tools.ietf.org/html/draft-vinod-carp-v1-03](https://tools.ietf.org/html/draft-vinod-carp-v1-03)

CARP's main draw is it's support for weights, which in real terms, allows for balancing load with consideration for heterogeneous hardware capacities.  For example a data center with servers of varying generations, that can handle respectively varying amounts of load.

### Load factor calculation:

Reference:
[https://tools.ietf.org/html/draft-vinod-carp-v1-03#section-3.3](https://tools.ietf.org/html/draft-vinod-carp-v1-03#section-3.3 )

![img](https://github.com/tinselcity/tinselcity.github.io/blob/master/images/carp.svg?raw=true "Load Factor Calculation")

#### Load factor calculation notes
- The "normalization" of the weight terms prior to performing the load calculation means the weights are relative to each other.
  For example `4, 4, 2, 3` == `400, 400, 200, 300`
- The normalized weights are ordered in ascending order by size from smallest normalized value to largest. eg. `0.1, 0.2, 0.2, 0.3...` -To a number that sums to `1.0`

### Backend selection:
Where "Backend" refers to servers behind the CARP'ing service receiving the proxied traffic.

Described in:

- [https://tools.ietf.org/html/draft-vinod-carp-v1-03#section-3.1](https://tools.ietf.org/html/draft-vinod-carp-v1-03#section-3.1)
- [https://tools.ietf.org/html/draft-vinod-carp-v1-03#section-3.2](https://tools.ietf.org/html/draft-vinod-carp-v1-03#section-3.2)
- [https://tools.ietf.org/html/draft-vinod-carp-v1-03#section-3.4](https://tools.ietf.org/html/draft-vinod-carp-v1-03#section-3.4)

Backend selection is done by combining:
- URI hash
- Backend group hash
- server load factor

The largest value is selected.  In psuedo code:

```sh
for backend in backend_list:

    hash = carp_combine_hash(uri_hash, backend_hash)
    weighted_hash = backend_weight * (double)hash
    
    if weighted_hash > max_hash:
        backend_selection = backend

```

Where `carp_combine_hash` is an `XOR` plus "large prime multiplication" plus "left shift" to promote diffusion and sparseness of hash function.

A potential 64 bit C++ implementation might look like:

```cpp
uint64_t carp64_combine_hash(uint64_t URL_Hash, uint64_t Backend_Hash)
{
        uint64_t Combined_Hash = (URL_Hash ^ Backend_Hash);
        Combined_Hash += Combined_Hash * CARP64_END_PRIME;
        Combined_Hash = ROTL64(Combined_Hash, 21);
        return Combined_Hash;
}
```

### Drawbacks

#### Scaling with Servers
The CARPing function scales with `O(n)` where `n` is the number of backend servers.  In a very large data center with hundreds or thousands of servers, this could become a bottleneck.

#### Popularity
Real internet traffic is not so random.  Some URL's are more popular than others, and the pattern of usage might look like a [zipf](https://en.wikipedia.org/wiki/Zipf%27s_law) distribution.  Accounting for this might mean load for popular URL's would have to be spread to other servers.  An approach to spreading load for popular URL's, could be to rotate between the top N highest random weighted servers, as opposed to just the first.  This, adds complexity, however, and state must be shared across CARP'ing servers in order to preserve consensus, about which servers popular URL's would be proxied to.

### Summary

In summary CARP is just one of many ways to shard load across servers.  It's an interesting approach to load balancing with accounting for physical server capacity.  Thank you to [Marcel Flores](https://twitter.com/theoldroad) for his all his help and intuition with load factor calculations.

### References:

1. [CARP IETF Draft](https://datatracker.ietf.org/doc/html/draft-vinod-carp-v1-03)
2. [Rendezvous Hashing](https://en.wikipedia.org/wiki/Rendezvous_hashing#Cache_Array_Routing_Protocol)

