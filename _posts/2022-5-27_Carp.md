---
layout: post
title: A Description of CARP
---

## Background
[Sharding](https://en.wikipedia.org/wiki/Shard_(database_architecture)) is a mechanism for distributing data or load in distributed systems design.  One described method for sharding or load balancing HTTP requests across proxy servers is ["Cache Array Routing Protocol"](https://en.wikipedia.org/wiki/Cache_Array_Routing_Protocol) (CARP).

CARP itself is form of [weighted](https://en.wikipedia.org/wiki/Rendezvous_hashing#Weighted_variations) [Rendezvous Hashing](https://en.wikipedia.org/wiki/Rendezvous_hashing), (also referred to as highest random weight (HRW)) which (from wiki):

- ```allows clients to achieve distributed agreement on a set of k options out of a possible set of n options.```

The general algorithm for:

1. calculating load factors (per server) using relative weights
2. selection based on uri hash + server hash + load factor

is described in the IETF draft: [https://tools.ietf.org/html/draft-vinod-carp-v1-03](https://tools.ietf.org/html/draft-vinod-carp-v1-03)

CARP's main draw is it's support for weights, which in real terms, allows for balancing load with consideration for heterogeneous hardware capacities.  For example a data center with servers of varying generations, that can handle respectively varying amounts of load.

#### Load factor calculation:
The "Load factor calculation" is the most _interesting_ aspect of CARP.

Reference:
[https://tools.ietf.org/html/draft-vinod-carp-v1-03#section-3.3](https://tools.ietf.org/html/draft-vinod-carp-v1-03#section-3.3 )

![img](images/carp.svg?raw=true "Load Factor Calculation")

##### Load factor calculation notes
- The "normalization" of the weight terms prior to performing the load calculation means the weights are relative to each other.
  For example `4, 4, 2, 3` == `400, 400, 200, 300`
- The normalized weights are ordered in ascending order by size from smallest normalized value to largest. eg. `0.1, 0.2, 0.2, 0.3...` -To a number that sums to `1.0`

#### Backend selection:
Where "Backend" refers to servers behind the CARP'ing service receiving the proxied traffic.

Described in:

- [https://tools.ietf.org/html/draft-vinod-carp-v1-03#section-3.1](https://tools.ietf.org/html/draft-vinod-carp-v1-03#section-3.1)
- [https://tools.ietf.org/html/draft-vinod-carp-v1-03#section-3.2](https://tools.ietf.org/html/draft-vinod-carp-v1-03#section-3.2)
- [https://tools.ietf.org/html/draft-vinod-carp-v1-03#section-3.4](https://tools.ietf.org/html/draft-vinod-carp-v1-03#section-3.4)

Backend selection is done by combining:
- URI hash
- Backend group hash (one per backend group -further per server per port selection is done with another CARP sans weight)
- server load factor

The largest value is selected.  In psuedo code:

```sh
for backend in backend_list:

    hash = carp_combine_hash(uri_hash, backend_hash)
    weighted_hash = backend_weight * (double)hash
    
    if weighted_hash > max_hash:
        backend_selection = backend

```

Where `carp_combine_hash` is an `XOR` plus "large prime multiplication" plus "left shift" to promote diffusion and sparseness of hash function.

A potential 64 bit C++ implementation might look like:

```cpp
uint64_t carp64_combine_hash(uint64_t URL_Hash, uint64_t Backend_Hash)
{
        uint64_t Combined_Hash = (URL_Hash ^ Backend_Hash);
        Combined_Hash += Combined_Hash * CARP64_END_PRIME;
        Combined_Hash = ROTL64(Combined_Hash, 21);
        return Combined_Hash;
}
```

#### Thanks!
Thank you to [Marcel Flores](https://www.linkedin.com/in/marcel-flores-31601814/) for his all his help and great intuition with load factor calculations.

#### References:
1. [CARP IETF Draft](https://datatracker.ietf.org/doc/html/draft-vinod-carp-v1-03)
2. [Rendezvous Hashing](https://en.wikipedia.org/wiki/Rendezvous_hashing#Cache_Array_Routing_Protocol)